{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the unstructured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from helpers.lookups import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of French stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/scraped_links.pkl', 'rb') as file:\n",
    "    scraped_links = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"https://usj.edu.lb/fp/rattaches.php?inst=41\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(test)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(link):\n",
    "    response = requests.get(link)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Fetch the description of the title\n",
    "    body = soup.find_all(\"p\")\n",
    "    paragraphs = []\n",
    "    for paragraph in body:\n",
    "        paragraphs.append(paragraph.text)\n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the list of strings\n",
    "def clean_text(data):\n",
    "    cleaned_data = []\n",
    "    for sentence in data:\n",
    "        # Tokenize the sentence\n",
    "        tokens = word_tokenize(sentence)\n",
    "        # Remove stopwords and convert to lowercase\n",
    "        cleaned_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "        # Join the cleaned tokens back into a sentence\n",
    "        cleaned_sentence = \" \".join(cleaned_tokens)\n",
    "        # Remove empty sentences\n",
    "        if cleaned_sentence:\n",
    "            cleaned_data.append(cleaned_sentence)\n",
    "    \n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "# Open the file in write mode to clean it\n",
    "with open(Path.ScrapedText.value, 'w') as file:\n",
    "    file.write('')  # Write an empty string to clear the file\n",
    "\n",
    "    \n",
    "for link in scraped_links:\n",
    "    text = extract_text(link)\n",
    "    cleaned_text = clean_text(text)\n",
    "\n",
    "    with open(Path.ScrapedText.value, 'a') as file:\n",
    "        file.write(link + \"\\n\")\n",
    "\n",
    "        for t in cleaned_text:\n",
    "            file.write(t + \"\\n\")    \n",
    "\n",
    "        file.write(\"------------------------------------------------------\\n\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
